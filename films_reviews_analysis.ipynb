{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27b4d3c-8d2f-4a7d-b157-45e6161cd3cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Подготовим все библиотеки и пропишем пути к файлам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a362e39-9608-44b5-91db-32f770beed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bfc780-19d9-4230-a2dd-924c1869ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799745e7-866d-44ef-9201-37dff6aadbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c6d10a-79ab-4967-85d8-ba56b78bbf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54cd28-9844-48e6-be1d-cac4eb20f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c05b154-b8bf-4da5-860d-d52aef444f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd57a0-5ac6-4775-876a-6a73fe7a7c8d",
   "metadata": {},
   "source": [
    "# Все необходимые библиотеки установлены, можно преступать к обработке данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd1a123-f0e6-4249-9d2f-e187a621acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import os\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from catboost import Pool, CatBoostClassifier, cv\n",
    "from catboost.text_processing import Tokenizer\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import string\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b728bf46-50ae-4764-a6c6-8d7e25740941",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ffc2477-36d9-4624-baaa-2654bb5083ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\krellQ\\\\PycharmProjects\\\\SOM\\\\sentiment_analysis'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3137a5e1-7a2f-4f9c-8b18-8e9747a1aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '\\\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9d47998-c98a-42ca-9310-25756ab912d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'neu', 'pos']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_path = os.path.join(dir_ + file_name)\n",
    "folder_path\n",
    "os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd2e926-4a6a-453f-a418-bf8956b92f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "tokenizer = Tokenizer(lowercasing=True,\n",
    "                      separator_type='ByDelimiter',\n",
    "                      token_types=['Word', 'Punctuation'],\n",
    "                      languages =['russian'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8514b14c-febd-47b3-b392-cf9fb5c3fdf7",
   "metadata": {},
   "source": [
    "# Рассмотрим идею работы с этими данными"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b7bd1-5f23-4f35-a105-41decc4855b7",
   "metadata": {},
   "source": [
    "Функции подготовки данных для градиентного бустинга стоит вынести отдельно т.к. библиотека Catboost предполагает использование встроенных инструментов векторизации и очистки текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a438a08-a9a2-4725-86ab-b5c6d8b0e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts):\n",
    "    return [tokenizer.tokenize(text) for text in texts]\n",
    "\n",
    "def remove_stop_words(texts, words):\n",
    "    texts_copy = []\n",
    "    words_set = set(words)\n",
    "    for text in tokenize_texts(texts):\n",
    "        text_copy = []\n",
    "        for token in text:\n",
    "            if token not in words_set:\n",
    "                text_copy.append(token)\n",
    "        texts_copy.append(' '.join(text_copy))\n",
    "    return texts_copy\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    morph = MorphAnalyzer()\n",
    "    words = text.split()\n",
    "    lemmatized_words = [morph.parse(word)[0].normal_form for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = ru_stopwords\n",
    "    filtered_text = ''.join([word for word in text if word.lower() not in stop_words])\n",
    "    return filtered_text\n",
    "\n",
    "def quantize_emotion(emotion):\n",
    "    if emotion == 'neg':\n",
    "        return -1\n",
    "    elif emotion == 'neu':\n",
    "        return 0\n",
    "    elif emotion == 'pos':\n",
    "        return 1\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4924fdfb-1ed4-4330-9743-acdba03c0959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_catboost_model(folder_path):\n",
    "    folder_path = folder_path + \"/\"\n",
    "    df = pd.DataFrame(columns=['text', 'sentiment'])\n",
    "    for directory in os.listdir(folder_path):\n",
    "        if os.path.isdir(folder_path + directory):\n",
    "            files = np.array(os.listdir(folder_path + directory))\n",
    "            for file in files:\n",
    "                with open(os.path.join(folder_path + directory + '/', file), encoding='utf-8') as f:\n",
    "                    review = f.read()\n",
    "                    review = remove_stopwords(review)\n",
    "                    current_df = pd.DataFrame({'text': [review], 'sentiment': directory})\n",
    "                    df = df._append(current_df, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    random_under_sampler = RandomUnderSampler(random_state = 0)\n",
    "    #df_review_bal, df_review_bal['sentiment'] = random_under_sampler.fit_resample(df[['text']], df['sentiment'])\n",
    "    #train, test = train_test_split(df_review_bal, test_size = 0.25, random_state = 42,)\n",
    "\n",
    "    train, test = train_test_split(df, test_size = 0.3, random_state = 0)\n",
    "    \n",
    "    #train['text'] = tokenize_texts(train['text'])\n",
    "    #test['text'] = tokenize_texts(test['text'])\n",
    "    \n",
    "    train_x, train_y = train['text'], train['sentiment']\n",
    "    test_x, test_y = test['text'], test['sentiment']\n",
    "\n",
    "    X_train = train_x.to_frame('text')\n",
    "    X_test = test_x.to_frame('text')\n",
    "    return X_train, train_y, X_test, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91794fa4-e1d3-4736-9793-16283bcdb170",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = prepare_data_for_catboost_model(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f6a6c73-febf-4e7c-b9e5-a43184155859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_catboost(X_train, X_test, y_train, y_test, catboost_params = {}, verbose = 100):\n",
    "    text_features = ['text']\n",
    "    learning_pool = Pool(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        text_features = text_features,\n",
    "        feature_names = list(X_train)\n",
    "    )\n",
    "    test_pool = Pool(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        text_features = text_features,\n",
    "        feature_names = list(X_train)\n",
    "    )\n",
    "    \n",
    "    catboost_default_params = {\n",
    "        'iterations': 1000,\n",
    "        'learning_rate': 0.03,\n",
    "        'eval_metric': 'TotalF1'\n",
    "    }\n",
    "    \n",
    "    catboost_default_params.update(catboost_params)\n",
    "    model = CatBoostClassifier(**catboost_default_params)\n",
    "    model.fit(learning_pool, eval_set=test_pool, verbose=verbose)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95c636b-4e17-4574-abcc-43e9363ff309",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = train_catboost(train_x,\n",
    "               test_x,\n",
    "               train_y,\n",
    "               test_y,\n",
    "               catboost_params={\n",
    "                'text_processing': [\n",
    "                'NaiveBayes+Word|BoW+Word,BiGram'\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e0dca7-c120-4414-87d6-42c2d1c873b9",
   "metadata": {},
   "source": [
    "Без удаления Stopwords получили F1:                                     \r\n",
    "bestTest 0.6598218546= \n",
    "При равном распределении выборки, но уменьшении общего объёма данных    bestTest = 0.6992715462\n",
    "После удаления:                                                         bestTest 0.672708907146"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aacd31-e23a-4f64-af93-afb85c71339f",
   "metadata": {},
   "source": [
    "Получились низкие показатели по сравнению с даже куда более простыми алгоритмами, но это связано в первую очередь с тем, что у меня не получилось\n",
    "имплементировать нормально векторизацию и алгоритм bag_of_words, который планировался в реализации, но первоочередной для меня было изучить \n",
    "принцип работы catboost и понять процесс работы алгоритмов обработки текста.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "42664d7e-1c2e-4912-909d-93243653c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(folder_path):\n",
    "    train_x, train_y, test_x, test_y = prepare_data_for_catboost_model(folder_path)\n",
    "    model = train_catboost(train_x,\n",
    "               test_x,\n",
    "               train_y,\n",
    "               test_y,\n",
    "               catboost_params={\n",
    "                'text_processing': [\n",
    "                'NaiveBayes+Word|BoW+Word,BiGram'\n",
    "        ]\n",
    "    }\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1629d29b-ff3d-429b-ae74-98b094400365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, folder_path):\n",
    "    result = []\n",
    "    for file in os.listdir(folder_path):\n",
    "         with open(os.path.join(folder_path + '/', file), encoding='utf-8') as f:\n",
    "             text = f.read()\n",
    "             ans = model.predict()\n",
    "             result.append(quantize_emotion(ans))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a63e3f6-8559-4555-b1f4-4637fce53052",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'claccification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclaccification_report\u001b[49m(my_model))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'claccification_report' is not defined"
     ]
    }
   ],
   "source": [
    "print(claccification_report(my_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311576f-8c86-4ec4-807c-1ca47de09f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
